{"nbformat":4,"nbformat_minor":0,"metadata":{"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3,"colab":{"name":"tutorial.ipynb","provenance":[],"collapsed_sections":["oMty1YwuWHpN"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/arthurflor23/spelling-correction/blob/master/src/tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"gP-v0E_S-mQP"},"source":["<img src=\"https://github.com/arthurflor23/spelling-correction/blob/master/doc/image/header.png?raw=true\">\n","\n","# Spelling Correction using TensorFlow 2.x\n","\n","This tutorial shows how you can use the project [Spelling Correction](https://github.com/arthurflor23/text-corretion) in your Google Colab.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"oMty1YwuWHpN"},"source":["## 1 Localhost Environment\n","\n","We'll make sure you have the project in your Google Drive with the datasets folders. If you already have structured files in the cloud, skip this step."]},{"cell_type":"markdown","metadata":{"id":"39blvPTPQJpt"},"source":["### 1.1 Datasets\n","\n","The datasets that you can use:\n","\n","a. [BEA2019](https://www.cl.cam.ac.uk/research/nl/bea2019st/)\n","\n","b. [Bentham](http://www.transcriptorium.eu/~tsdata/)\n","\n","c. [CoNLL13](https://www.comp.nus.edu.sg/~nlp/conll13st.html)\n","\n","d. [CoNLL14](https://www.comp.nus.edu.sg/~nlp/conll14st.html)\n","\n","e. [Google](https://ai.google/research/pubs/pub41880)\n","\n","f. [IAM](http://www.fki.inf.unibe.ch/databases/iam-handwriting-database)\n","\n","g. [Rimes](http://www.a2ialab.com/doku.php?id=rimes_database:start)\n","\n","h. [Saint Gall](https://fki.tic.heia-fr.ch/databases/saint-gall-database)\n","\n","i. [Washington](https://fki.tic.heia-fr.ch/databases/washington-database)"]},{"cell_type":"markdown","metadata":{"id":"QVBGMLifWQwl"},"source":["### 1.2 Raw folder\n","\n","On localhost, download the code project from GitHub and extract the chosen dataset in the **raw** folder. Don't change anything of the structure of the dataset, since the scripts were made from the **original structure** of them. Your project directory will be like this:\n","\n","```\n",".\n","├── raw\n","│   ├── bea2019\n","│   │   ├── json\n","│   │   ├── json_to_m2.py\n","│   │   ├── licence.wi.txt\n","│   │   ├── license.locness.txt\n","│   │   ├── m2\n","│   │   └── readme.txt\n","│   ├── bentham\n","│   │   ├── BenthamDatasetR0-GT\n","│   │   └── BenthamDatasetR0-Images\n","│   ├── conll13\n","│   │   ├── m2scorer\n","│   │   ├── original\n","│   │   ├── README\n","│   │   ├── revised\n","│   │   └── scripts\n","│   ├── conll14\n","│   │   ├── alt\n","│   │   ├── noalt\n","│   │   ├── README\n","│   │   └── scripts\n","│   ├── google\n","│   │   ├── europarl-v6.cs\n","│   │   ├── europarl-v6.de\n","│   │   ├── europarl-v6.en\n","│   │   ├── europarl-v6.es\n","│   │   ├── europarl-v6.fr\n","│   │   ├── news.2007.cs.shuffled\n","│   │   ├── news.2007.de.shuffled\n","│   │   ├── news.2007.en.shuffled\n","│   │   ├── news.2007.es.shuffled\n","│   │   ├── news.2007.fr.shuffled\n","│   │   ├── news.2008.cs.shuffled\n","│   │   ├── news.2008.de.shuffled\n","│   │   ├── news.2008.en.shuffled\n","│   │   ├── news.2008.es.shuffled\n","│   │   ├── news.2008.fr.shuffled\n","│   │   ├── news.2009.cs.shuffled\n","│   │   ├── news.2009.de.shuffled\n","│   │   ├── news.2009.en.shuffled\n","│   │   ├── news.2009.es.shuffled\n","│   │   ├── news.2009.fr.shuffled\n","│   │   ├── news.2010.cs.shuffled\n","│   │   ├── news.2010.de.shuffled\n","│   │   ├── news.2010.en.shuffled\n","│   │   ├── news.2010.es.shuffled\n","│   │   ├── news.2010.fr.shuffled\n","│   │   ├── news.2011.cs.shuffled\n","│   │   ├── news.2011.de.shuffled\n","│   │   ├── news.2011.en.shuffled\n","│   │   ├── news.2011.es.shuffled\n","│   │   ├── news.2011.fr.shuffled\n","│   │   ├── news-commentary-v6.cs\n","│   │   ├── news-commentary-v6.de\n","│   │   ├── news-commentary-v6.en\n","│   │   ├── news-commentary-v6.es\n","│   │   └── news-commentary-v6.fr\n","│   ├── iam\n","│   │   ├── ascii\n","│   │   ├── forms\n","│   │   ├── largeWriterIndependentTextLineRecognitionTask\n","│   │   ├── lines\n","│   │   └── xml\n","│   ├── rimes\n","│   │   ├── eval_2011\n","│   │   ├── eval_2011_annotated.xml\n","│   │   ├── training_2011\n","│   │   └── training_2011.xml\n","│   ├── saintgall\n","│   │   ├── data\n","│   │   ├── ground_truth\n","│   │   ├── README.txt\n","│   │   └── sets\n","│   └── washington\n","│       ├── data\n","│       ├── ground_truth\n","│       ├── README.txt\n","│       └── sets\n","└── src\n","    ├── data\n","    │   ├── evaluation.py\n","    │   ├── generator.py\n","    │   ├── __init__.py\n","    │   ├── preproc.py\n","    │   └── reader.py\n","    ├── main.py\n","    ├── tool\n","    │   ├── __init__.py\n","    │   ├── seq2seq.py\n","    │   ├── statistical.py \n","    │   └── transformer.py\n","    └── tutorial.ipynb\n","\n","```\n","\n","After that, create virtual environment and install the dependencies with python 3 and pip:\n","\n","> ```python -m venv .venv && source .venv/bin/activate```\n","\n","> ```pip install -r requirements.txt```"]},{"cell_type":"markdown","metadata":{"id":"WyLRbAwsWSYA"},"source":["### 1.3 Dataset folders\n","\n","Now, you'll run the *transform* function from **main.py**. For this, execute on **src** folder:\n","\n","> ```python main.py --source=<DATASET_NAME> --transform```\n","\n","Your data will be preprocess and encode, creating and saving in the **data** folder. Now your project directory will be like this:\n","\n","\n","```\n",".\n","├── data\n","│   ├── bea2019.txt\n","│   ├── bentham.txt\n","│   ├── conll13.txt\n","│   ├── conll14.txt\n","│   ├── google.txt\n","│   ├── iam.txt\n","│   ├── rimes.txt\n","│   ├── saintgall.txt\n","│   └── washington.txt\n","├── raw\n","│   ├── bea2019\n","│   │   ├── json\n","│   │   ├── json_to_m2.py\n","│   │   ├── licence.wi.txt\n","│   │   ├── license.locness.txt\n","│   │   ├── m2\n","│   │   └── readme.txt\n","│   ├── bentham\n","│   │   ├── BenthamDatasetR0-GT\n","│   │   └── BenthamDatasetR0-Images\n","│   ├── conll13\n","│   │   ├── m2scorer\n","│   │   ├── original\n","│   │   ├── README\n","│   │   ├── revised\n","│   │   └── scripts\n","│   ├── conll14\n","│   │   ├── alt\n","│   │   ├── noalt\n","│   │   ├── README\n","│   │   └── scripts\n","│   ├── google\n","│   │   ├── europarl-v6.cs\n","│   │   ├── europarl-v6.de\n","│   │   ├── europarl-v6.en\n","│   │   ├── europarl-v6.es\n","│   │   ├── europarl-v6.fr\n","│   │   ├── news.2007.cs.shuffled\n","│   │   ├── news.2007.de.shuffled\n","│   │   ├── news.2007.en.shuffled\n","│   │   ├── news.2007.es.shuffled\n","│   │   ├── news.2007.fr.shuffled\n","│   │   ├── news.2008.cs.shuffled\n","│   │   ├── news.2008.de.shuffled\n","│   │   ├── news.2008.en.shuffled\n","│   │   ├── news.2008.es.shuffled\n","│   │   ├── news.2008.fr.shuffled\n","│   │   ├── news.2009.cs.shuffled\n","│   │   ├── news.2009.de.shuffled\n","│   │   ├── news.2009.en.shuffled\n","│   │   ├── news.2009.es.shuffled\n","│   │   ├── news.2009.fr.shuffled\n","│   │   ├── news.2010.cs.shuffled\n","│   │   ├── news.2010.de.shuffled\n","│   │   ├── news.2010.en.shuffled\n","│   │   ├── news.2010.es.shuffled\n","│   │   ├── news.2010.fr.shuffled\n","│   │   ├── news.2011.cs.shuffled\n","│   │   ├── news.2011.de.shuffled\n","│   │   ├── news.2011.en.shuffled\n","│   │   ├── news.2011.es.shuffled\n","│   │   ├── news.2011.fr.shuffled\n","│   │   ├── news-commentary-v6.cs\n","│   │   ├── news-commentary-v6.de\n","│   │   ├── news-commentary-v6.en\n","│   │   ├── news-commentary-v6.es\n","│   │   └── news-commentary-v6.fr\n","│   ├── iam\n","│   │   ├── ascii\n","│   │   ├── forms\n","│   │   ├── largeWriterIndependentTextLineRecognitionTask\n","│   │   ├── lines\n","│   │   └── xml\n","│   ├── rimes\n","│   │   ├── eval_2011\n","│   │   ├── eval_2011_annotated.xml\n","│   │   ├── training_2011\n","│   │   └── training_2011.xml\n","│   ├── saintgall\n","│   │   ├── data\n","│   │   ├── ground_truth\n","│   │   ├── README.txt\n","│   │   └── sets\n","│   └── washington\n","│       ├── data\n","│       ├── ground_truth\n","│       ├── README.txt\n","│       └── sets\n","└── src\n","    ├── data\n","    │   ├── evaluation.py\n","    │   ├── generator.py\n","    │   ├── __init__.py\n","    │   ├── preproc.py\n","    │   └── reader.py\n","    ├── main.py\n","    ├── tool\n","    │   ├── __init__.py\n","    │   ├── seq2seq.py\n","    │   ├── statistical.py \n","    │   └── transformer.py\n","    └── tutorial.ipynb\n","\n","```\n","\n","Then upload the **data** and **src** folders in the same directory in your Google Drive."]},{"cell_type":"markdown","metadata":{"id":"jydsAcWgWVth"},"source":["## 2 Google Drive Environment\n"]},{"cell_type":"markdown","metadata":{"id":"wk3e7YJiXzSl"},"source":["### 2.1 TensorFlow 2.x"]},{"cell_type":"markdown","metadata":{"id":"Z7twXyNGXtbJ"},"source":["Make sure the jupyter notebook is using GPU mode."]},{"cell_type":"code","metadata":{"id":"mHw4tODULT1Z"},"source":["!nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w5ukHtpZiz0g"},"source":["%tensorflow_version 2.x\n","import tensorflow as tf\n","\n","device_name = tf.test.gpu_device_name()\n","\n","if device_name != \"/device:GPU:0\":\n","    raise SystemError(\"GPU device not found\")\n","\n","print(\"Found GPU at: {}\".format(device_name))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FyMv5wyDXxqc"},"source":["### 2.2 Google Drive"]},{"cell_type":"markdown","metadata":{"id":"P5gj6qwoX9W3"},"source":["Mount your Google Drive partition.\n","\n","**Note:** *\\\"Colab Notebooks/spelling-correction/src/\\\"* was the directory where you put the project folders, specifically the **src** folder."]},{"cell_type":"code","metadata":{"id":"ACQn1iBF9k9O"},"source":["from google.colab import drive\n","\n","drive.mount(\"./gdrive\", force_remount=True)\n","\n","%cd \"./gdrive/My Drive/Colab Notebooks/spelling-correction/src/\"\n","!ls -l"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YwogUA8RZAyp"},"source":["After mount, you can see the list os files in the project folder."]},{"cell_type":"markdown","metadata":{"id":"-fj7fSngY1IX"},"source":["## 3 Set Python Classes"]},{"cell_type":"markdown","metadata":{"id":"p6Q4cOlWhNl3"},"source":["### 3.1 Environment"]},{"cell_type":"markdown","metadata":{"id":"wvqL2Eq5ZUc7"},"source":["First, let's define our environment variables.\n","\n","Set the main configuration parameters, such as dataset, method, number of epochs and batch size. This make compatible with **main.py** and jupyter notebook:\n","\n","* **dataset**:\n","\n","  * **``bea2019``**, **``bentham``**, **``conll13``**, **``conll14``**, **``google``**, **``iam``**, **``rimes``**, **``saintgall``**, **``washington``**\n","\n","* **mode**:\n","\n","  * neural network: **``luong``**, **``bahdanau``**, **``transformer``**\n","\n","  * statistical (localhost only): **``similarity``**, **``norvig``**, **``symspell``**\n","\n","* **epochs**: number of epochs\n","\n","* **batch_size**: number size of the batch"]},{"cell_type":"code","metadata":{"id":"_Qpr3drnGMWS"},"source":["import os\n","import datetime\n","import string\n","\n","# define parameters\n","source = \"bea2019\"\n","mode = \"luong\"\n","epochs = 1000\n","batch_size = 64\n","\n","# define paths\n","data_path = os.path.join(\"..\", \"data\")\n","source_path = os.path.join(data_path, f\"{source}.txt\")\n","\n","output_path = os.path.join(\"..\", \"output\", source, mode)\n","target_path = os.path.join(output_path, \"checkpoint_weights.hdf5\")\n","os.makedirs(output_path, exist_ok=True)\n","\n","# define number max of chars per line and list of valid chars\n","max_text_length = 128\n","charset_base = string.printable[:95]\n","charset_special = \"\"\"ÀÁÂÃÄÅÇÈÉÊËÌÍÎÏÑÒÓÔÕÖÙÚÛÜÝàáâãäåçèéêëìíîïñòóôõöùúûüý\"\"\"\n","\n","print(\"output\", output_path)\n","print(\"target\", target_path)\n","print(\"charset:\", charset_base + charset_special)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BFextshOhTKr"},"source":["### 3.2 DataGenerator Class"]},{"cell_type":"markdown","metadata":{"id":"KfZ1mfvsanu1"},"source":["The second class is **DataGenerator()**, responsible for:\n","\n","* Load the dataset partitions (train, valid, test);\n","\n","* Manager batchs for train/validation/test process."]},{"cell_type":"code","metadata":{"id":"8k9vpNzMIAi2"},"source":["from data.generator import DataGenerator\n","\n","dtgen = DataGenerator(source=source_path,\n","                      batch_size=batch_size,\n","                      charset=(charset_base + charset_special),\n","                      max_text_length=max_text_length)\n","\n","print(f\"Train sentences: {dtgen.size['train']}\")\n","print(f\"Validation sentences: {dtgen.size['valid']}\")\n","print(f\"Test sentences: {dtgen.size['test']}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-OdgNLK0hYAA"},"source":["### 3.3 Neural Network Model"]},{"cell_type":"markdown","metadata":{"id":"jHktk8AFcnKy"},"source":["In this step, the model will be created/loaded and default callbacks setup."]},{"cell_type":"code","metadata":{"id":"nV0GreStISTR"},"source":["from data import preproc as pp, evaluation as ev\n","from tool.seq2seq import Seq2SeqAttention\n","from tool.transformer import Transformer\n","\n","if mode == \"transformer\":\n","    # disable one hot encode (seq2seq) to use transformer model\n","    dtgen.one_hot_process = False\n","    model = Transformer(dtgen.tokenizer,\n","                        num_layers=6,\n","                        units=512,\n","                        d_model=256,\n","                        num_heads=8,\n","                        dropout=0.1,\n","                        stop_tolerance=20,\n","                        reduce_tolerance=15)\n","else:\n","    model = Seq2SeqAttention(dtgen.tokenizer,\n","                             mode,\n","                             units=512,\n","                             dropout=0.2,\n","                             stop_tolerance=20,\n","                             reduce_tolerance=15)\n","\n","model.compile(learning_rate=0.001)\n","model.summary(output_path, \"summary.txt\")\n","\n","# get default callbacks list and load checkpoint weights file (HDF5) if exists \n","model.load_checkpoint(target=target_path)\n","\n","callbacks = model.get_callbacks(logdir=output_path, checkpoint=target_path, verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T1fnz0Eugqru"},"source":["## 4 Training"]},{"cell_type":"markdown","metadata":{"id":"w1mLOcqYgsO-"},"source":["The training process using *fit_generator()* to fit memory. After training, the information (epochs and minimum loss) is save."]},{"cell_type":"code","metadata":{"id":"2P6MSoxCISlD"},"source":["# to calculate total and average time per epoch\n","start_time = datetime.datetime.now()\n","\n","h = model.fit(x=dtgen.next_train_batch(),\n","              epochs=epochs,\n","              steps_per_epoch=dtgen.steps['train'],\n","              validation_data=dtgen.next_valid_batch(),\n","              validation_steps=dtgen.steps['valid'],\n","              callbacks=callbacks,\n","              shuffle=True,\n","              verbose=1)\n","\n","total_time = datetime.datetime.now() - start_time\n","\n","loss = h.history['loss']\n","accuracy = h.history['accuracy']\n","\n","val_loss = h.history['val_loss']\n","val_accuracy = h.history['val_accuracy']\n","\n","time_epoch = (total_time / len(loss))\n","total_item = (dtgen.size['train'] + dtgen.size['valid'])\n","best_epoch_index = val_loss.index(min(val_loss))\n","\n","t_corpus = \"\\n\".join([\n","    f\"Total train sentences:      {dtgen.size['train']}\",\n","    f\"Total validation sentences: {dtgen.size['valid']}\",\n","    f\"Batch:                      {dtgen.batch_size}\\n\",\n","    f\"Total epochs:               {len(accuracy)}\",\n","    f\"Total time:                 {total_time}\",\n","    f\"Time per epoch:             {time_epoch}\",\n","    f\"Time per item:              {time_epoch / total_item}\\n\",\n","    f\"Best epoch                  {best_epoch_index + 1}\",\n","    f\"Training loss:              {loss[best_epoch_index]:.8f}\",\n","    f\"Training accuracy:          {accuracy[best_epoch_index]:.8f}\\n\",\n","    f\"Validation loss:            {val_loss[best_epoch_index]:.8f}\",\n","    f\"Validation accuracy:        {val_accuracy[best_epoch_index]:.8f}\"\n","])\n","\n","with open(os.path.join(output_path, \"train.txt\"), \"w\") as lg:\n","    lg.write(t_corpus)\n","    print(t_corpus)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"13g7tDjWgtXV"},"source":["## 5 Predict and Evaluate"]},{"cell_type":"markdown","metadata":{"id":"ddO26OT-g_QK"},"source":["Since the goal is to correct text, the metrics (CER, WER and SER) are calculated before and after of the correction.\n","\n","The predict process also using the *predict_generator()*:"]},{"cell_type":"code","metadata":{"id":"a9iHL6tmaL_j"},"source":["start_time = datetime.datetime.now()\n","\n","predicts = model.predict(x=dtgen.next_test_batch(), steps=dtgen.steps['test'], verbose=1)\n","predicts = [pp.text_standardize(x) for x in predicts]\n","\n","total_time = datetime.datetime.now() - start_time\n","\n","# calculate metrics (before and after)\n","old_metric, new_metric = ev.ocr_metrics(ground_truth=dtgen.dataset['test']['gt'],\n","                                        data=dtgen.dataset['test']['dt'],\n","                                        predict=predicts)\n","\n","# generate report\n","e_corpus = \"\\n\".join([\n","    f\"Total test sentences: {dtgen.size['test']}\\n\",\n","    f\"Total time:           {total_time}\",\n","    f\"Time per item:        {total_time / dtgen.size['test']}\\n\",\n","    f\"Metrics (before):\",\n","    f\"Character Error Rate: {old_metric[0]:.8f}\",\n","    f\"Word Error Rate:      {old_metric[1]:.8f}\",\n","    f\"Sequence Error Rate:  {old_metric[2]:.8f}\\n\",\n","    f\"Metrics (after):\",\n","    f\"Character Error Rate: {new_metric[0]:.8f}\",\n","    f\"Word Error Rate:      {new_metric[1]:.8f}\",\n","    f\"Sequence Error Rate:  {new_metric[2]:.8f}\"\n","])\n","\n","p_corpus = []\n","for i in range(dtgen.size['test']):\n","    p_corpus.append(f\"GT {dtgen.dataset['test']['gt'][i]}\")\n","    p_corpus.append(f\"DT {dtgen.dataset['test']['dt'][i]}\")\n","    p_corpus.append(f\"PD {predicts[i]}\\n\")\n","\n","# write report\n","with open(os.path.join(output_path, \"predict.txt\"), \"w\") as lg:\n","    lg.write(\"\\n\".join(p_corpus))\n","    print(\"\\n\".join(p_corpus[:30]))\n","\n","with open(os.path.join(output_path, \"evaluate.txt\"), \"w\") as lg:\n","    lg.write(e_corpus)\n","    print(e_corpus)"],"execution_count":null,"outputs":[]}]}